{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5908d3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea88b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "# ! Mnist is already integrated to TF :-) \n",
    "# In fact it's integrated in Keras dataset\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "# x = image , y = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d119ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x_train))\n",
    "print(len(x_test))\n",
    "# Just to ensure we have the right dataset\n",
    "# 60000 train images , 10000 test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61b24ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[0] # Array of images (28x28 array)\n",
    "# (0 = black, 255 = white opposite to EMNIST !!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40f57a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_train[0], cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79088efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e9ec9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_test[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ed3ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's play with layers (thanks to Keras !)\n",
    "# Since TF 2.0 Keras is intimatly mixed to TF\n",
    "# And devs encourages to use more Keras functions than just pure TF\n",
    "# convolution , flatten, dense\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense\n",
    "# we need to discuss of this too \n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae73c95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cf Conv2D Keras docs fo full list of arguments\n",
    "# Conv2D(filters, kernel_size, activation)\n",
    "# Flatten() turn a 2d images into a 1d array of 784 pixels (28*28)\n",
    "# Dense(neuros, activation = 'relu or softmax at the very least')\n",
    "\n",
    "class MNISTModel(Model):\n",
    "    def __init__(self):\n",
    "        super (MNISTModel, self).__init__()\n",
    "        self.conv1 = Conv2D(32,3, activation='relu')\n",
    "        self.flatten = Flatten()\n",
    "        self.dense1 = Dense(128, activation='relu')\n",
    "        #(32 --> 128 for every outputs we have 4 neurons)\n",
    "        # 128 we may increase the precision but time increases too\n",
    "        self.dense2 = Dense(10, activation='softmax')\n",
    "        # we have 10 digits so 10 neurons\n",
    "        # and format encoding is [0 0 0 0 0 0 0 0  1 0 ]\n",
    "        # read the course pdf !!!\n",
    "        \n",
    "    def call(self, x):\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.flatten(x1)\n",
    "        x3 = self.dense1(x2)\n",
    "        return self.dense2(x3)\n",
    "    \n",
    "model = MNISTModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5b85d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function & optimizer\n",
    "loss_function = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "# Caterogical losses\n",
    "optimizer = tf.keras.optimizers.Adam() #Instead of simple gradient descent optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8d547e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851525af",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f795a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inputs, outputs): # correct answers\n",
    "    with tf.GradientTape() as tape: # Change the Bias \n",
    "        predictions = model.call(inputs)\n",
    "        loss = loss_function(outputs, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    # tests\n",
    "    train-loss(loss) # should decrese quickly\n",
    "    train_accuracy(output, predictions)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033002b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(inputs, outputs): # correct answers\n",
    "    predictions = model.call(inputs)\n",
    "    loss = loss_function(outputs, predictions)\n",
    "    \n",
    "    \n",
    "    train-loss(loss) # should decrese quickly\n",
    "    train_accuracy(output, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90edc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatting and normalizing the dataset\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "# add a new dimension to the array\n",
    "x_train = x_train[..., tf.newaxis]\n",
    "x_test = x_test[..., tf.newaxis]\n",
    "# now we have 4 dimensions (read tf docs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd810ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74353cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's shuffle !!!\n",
    "# this is specially helpfull depending on how we collected the data\n",
    "# Ex. all the 0 then all the 1 and so on\n",
    "# Shuffling add a bit of randomness, anyway our model we'll  learn only 0 then 1\n",
    "train_data = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train, y_train)).shuffle(10000).batch(32)\n",
    "\n",
    "test_data = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_test, y_test)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79090bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-19 22:02:02.743147: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX\n",
      "2021-11-19 22:02:02.766251: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2491860000 Hz\n",
      "2021-11-19 22:02:02.767020: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5650b692e2d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-11-19 22:02:02.767093: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-11-19 22:02:02.767521: I tensorflow/core/common_runtime/process_util.cc:147] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x7f0c6370b170> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <function train_step at 0x7f0c6370b170> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:Layer mnist_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method MNISTModel.call of <__main__.MNISTModel object at 0x7f0c667d2ad0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method MNISTModel.call of <__main__.MNISTModel object at 0x7f0c667d2ad0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method MNISTModel.call of <__main__.MNISTModel object at 0x7f0c667d2ad0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method MNISTModel.call of <__main__.MNISTModel object at 0x7f0c667d2ad0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <function test_step at 0x7f0c6370b320> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <function test_step at 0x7f0c6370b320> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method MNISTModel.call of <__main__.MNISTModel object at 0x7f0c667d2ad0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method MNISTModel.call of <__main__.MNISTModel object at 0x7f0c667d2ad0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method MNISTModel.call of <__main__.MNISTModel object at 0x7f0c667d2ad0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method MNISTModel.call of <__main__.MNISTModel object at 0x7f0c667d2ad0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "Epochs: 1, Train loss: 0.13502880930900574, Train accuracy: 0.9590499997138977, Test loss: 0.05457510054111481 Test accuracy: 0.9815999865531921\n",
      "Epochs: 2, Train loss: 0.04188847541809082, Train accuracy: 0.987333357334137, Test loss: 0.043928105384111404 Test accuracy: 0.984000027179718\n",
      "Epochs: 3, Train loss: 0.02117619849741459, Train accuracy: 0.9931333065032959, Test loss: 0.05553993210196495 Test accuracy: 0.9815999865531921\n",
      "Epochs: 4, Train loss: 0.013594137504696846, Train accuracy: 0.9953166842460632, Test loss: 0.061977747827768326 Test accuracy: 0.9824000000953674\n",
      "Epochs: 5, Train loss: 0.008804754354059696, Train accuracy: 0.9969833493232727, Test loss: 0.06357469409704208 Test accuracy: 0.984000027179718\n"
     ]
    }
   ],
   "source": [
    "# Warning dûe to a bug in Jupyter notebook we need to copu everything here\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "x_train = x_train[..., tf.newaxis]\n",
    "x_test = x_test[..., tf.newaxis]\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train, y_train)).shuffle(10000).batch(32)\n",
    "test_data = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_test, y_test)).batch(32)\n",
    "\n",
    "class MNISTModel(Model):\n",
    "    def __init__(self):\n",
    "        super(MNISTModel, self).__init__()\n",
    "        self.conv1 = Conv2D(32, 3, activation='relu')\n",
    "        self.flatten = Flatten()\n",
    "        self.dense1 = Dense(128, activation='relu')\n",
    "        self.dense2 = Dense(10, activation='softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.flatten(x1)\n",
    "        x3 = self.dense1(x2)\n",
    "        return self.dense2(x3)\n",
    "    \n",
    "model = MNISTModel()\n",
    "\n",
    "loss_function = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "@tf.function\n",
    "def train_step(inputs, outputs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs)\n",
    "        loss = loss_function(outputs, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    train_loss(loss)\n",
    "    train_accuracy(outputs, predictions)\n",
    "    \n",
    "@tf.function\n",
    "def test_step(inputs, outputs):\n",
    "    predictions = model(inputs)\n",
    "    loss = loss_function(outputs, predictions)\n",
    "    \n",
    "    test_loss(loss)\n",
    "    test_accuracy(outputs, predictions)\n",
    "    \n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for train_inputs, train_labels in train_data:\n",
    "        train_step(train_inputs, train_labels)\n",
    "        \n",
    "    for test_inputs, test_labels in test_data:\n",
    "        test_step(test_inputs, test_labels)\n",
    "        \n",
    "    template = 'Epochs: {}, Train loss: {}, Train accuracy: {}, Test loss: {} Test accuracy: {}'\n",
    "    print(template.format(\n",
    "        epoch + 1,\n",
    "        train_loss.result(),\n",
    "        train_accuracy.result(),\n",
    "        test_loss.result(),\n",
    "        test_accuracy.result(),\n",
    "    ))\n",
    "    \n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8719fb17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
